# PySpark Exercises

A comprehensive collection of exercises and mini-projects using [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) (Python API for Apache Spark). These materials were developed as part of Udacity's [Learn Spark at Udacity](https://www.udacity.com/course/learn-spark-at-udacity--ud2002) course, providing hands-on experience with Apache Spark's core features and advanced capabilities.

## ğŸ›  Tech Stack
- Python
- PySpark
- NumPy
- pandas
- Matplotlib
- Jupyter Notebook
- AWS
- GitHub

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ data_wrangling_with_spark/          # Data processing fundamentals
â”‚   â”œâ”€â”€ notebooks covering procedural vs functional programming
â”‚   â”œâ”€â”€ Spark operations and lazy evaluation
â”‚   â”œâ”€â”€ DataFrame operations and SQL
â”‚   â””â”€â”€ practice datasets
â”œâ”€â”€ debugging_and_optimization/          # Performance tuning
â”‚   â””â”€â”€ exercises/
â”‚       â”œâ”€â”€ data skewness handling
â”‚       â”œâ”€â”€ broadcast joins
â”‚       â””â”€â”€ repartitioning strategies
â”œâ”€â”€ machine_learning_with_spark/         # ML implementations
â”‚   â”œâ”€â”€ feature engineering
â”‚   â”œâ”€â”€ linear regression
â”‚   â”œâ”€â”€ k-means clustering
â”‚   â””â”€â”€ model tuning
â””â”€â”€ setting_up_spark_clusters_with_aws/  # AWS deployment
    â”œâ”€â”€ demo_code/
    â””â”€â”€ exercises/
        â”œâ”€â”€ EMR cluster creation
        â”œâ”€â”€ script submission
        â””â”€â”€ S3 integration
```

## ğŸ“š Course Content

### 1. The Power of Spark
- Introduction to Big Data ecosystem
- MapReduce implementation
- Fundamental Spark concepts

### 2. Data Wrangling with Spark
- Functional programming principles
- DataFrame operations and transformations
- Spark SQL integration
- Data input/output operations

### 3. Setting up Spark Clusters with AWS
- EMR cluster deployment
- AWS CLI integration
- S3 data storage
- Spark job submission

### 4. Debugging and Optimization
- Data skewness handling
- Broadcast join optimization
- Partition management
- Performance tuning strategies

### 5. Machine Learning with Spark
- Feature engineering (numeric and text)
- Linear regression implementation
- K-means clustering
- Model tuning and optimization
- ML pipeline construction

## ğŸš€ Getting Started

1. **Environment Setup**
   - Follow PySpark's official [installation guide](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)
   - Set up Python environment with required dependencies
   - Configure AWS credentials (for cluster-related exercises)

2. **Running the Exercises**
   - Each directory contains Jupyter notebooks and Python scripts
   - Start with the numbered notebooks in each section
   - Solutions are provided for self-assessment

## ğŸ“ Notes
- Exercise solutions are available in corresponding `*_solution` notebooks
- AWS-related exercises require active AWS credentials
- Sample datasets are included in respective directories

## ğŸ¤ Contributing
Feel free to submit issues and enhancement requests!
