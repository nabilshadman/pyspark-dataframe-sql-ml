# PySpark Exercises
A collection of **exercises** (and mini projects) with [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) (Python API for Apache Spark). These exercises were completed as part of Udacity's [course](https://www.udacity.com/course/learn-spark-at-udacity--ud2002) on Apache Spark. Overall, this course provides an introduction to Apache Spark, and develops skills with Spark DataFrames, Spark SQL, Spark ML, and Spark Clusters with AWS.  

**Tech Stack:** Python, NumPy, pandas, Matplotlib, PySpark, Jupyter Notebook, GitHub, AWS.  

##  Learning Outcomes 
The learning **outcomes** of the course (and associated exercises) are described below.  

- Introduction to the Course
    - In this lesson, we learn more about the course - what is covered, and some background about the instructors (David Drummond, Judit Lantos).
- The Power of Spark
    - In this lesson, we learn about the problems that Apache Spark is designed to solve. We also learn about the greater Big Data ecosystem and how Spark fits into it.
- Data Wrangling with Spark
    - In this lesson, we dive into how to use Spark for cleaning and aggregating data.
- Setting up Spark Clusters with AWS
    - In this lesson, we learn to run Spark on a distributed cluster in AWS UI and AWS CLI.
- Debugging and Optimization
    - In this lesson, we learn best practices for debugging and optimizing Spark applications.
- Machine Learning with Spark
    - In this lesson, we explore Spark's ML capabilities and build ML models and pipelines.

 ## Environment
 For setting up the software **environment** to run the exercise scripts and notebooks, please refer to PySpark's installation [info](https://spark.apache.org/docs/latest/api/python/getting_started/install.html).  
 
